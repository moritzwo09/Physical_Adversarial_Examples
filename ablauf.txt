###################### Grundidee #########################

1. Adversarial Examples ausdrucken und testen, ob sie nach Wiederaufnahme immer noch adversarial sind
2. andere Angriffsarten versuchen, die tatsächliche Veränderungen (nicht nur rauschen) verursachen - wahrscheinlich basierend auf dem Expectation over Transformation Framework oder Robust Physical Perturbations
3. Evaluation von Gegenmaßnahmen

##################### Meeting 17.10. #####################


- Vorher normale Schilder ausdrucken und schauen wie gut das Netz die Schilder erkennt als Baseline
- mit Stoppschildern anfangen evtl ein weiteres Schild
- Mal nachschaue welches Format die Bilder haben und mit welchem Drucker und auf welches Material gedruckt wurde

- dann Expectation over Transformation

- Gegenmaßnahme: LogicOOD



########### Fragen ##############
1. wie funktioniert das mit der Größe der Bilder?
 - Auf welcher Größe druckt man sie aus? 64x64? 512x512?
 - Resized man sie, wenn das Modell sie klassifizieren soll?
 => Lösung: Morgilus et al referencing DARTS:
    1. have high-res image and generate mask which include only the sign
    2. resize both mask and image to classifiers input dimensions (32x32).
    3. do adv. attack
    4. resize perturbation and add to original image


############################## Was gemacht? ####################################

1.Versuch

- Bilder auf 1200x1200 auf DIN-A4 ausgedruckt. 
- Die Bilder an die Tür geklebt und mit einem Abstand von 1-4 Metern Fotos gemacht (mit drehen der Kamera) (mitten am Tag - hell)
- Bilder auf MacBook gesendet (airdrop)
- die .HEIC files in png umgewandelt (imagemagick) oder als png angezeigt (pillow_heif)
=> die Qualität war viel zu schlecht, es war viel zu weit weg

Idee: erst mal irgendwie zum Laufen bringen, deshalb einfache Bilder von nah dran
Schwierigkeit: low-res Bilder auf DINA-4 drucken, und dann abfotografieren und dann wieder runterskalieren, damit das Modell das nutzen kann


2. Versuch

- Bilder auf 1200x1200 auf DIN-A4 ausgedruckt. 
- Bilder an die Tür geklebt und von sehr nah (3 Fotos pro Bild aus verschiedenen Weiten ca. 30-50cm entfernt) abfotografieren (Zimmerlicht, weil dunkel)
- Bilder auf MacBook senden (Airdrop)
- die .HEIC files in png umgewandelt (imagemagick) oder als png angezeigt (pillow_heif)
=> besser

Resultat nach Prediction:
- 66% accuracy
- er tut sich schwer mit den Bildern, die etwas weiter weg sind. Von denen, die nah dran sind hat er alle richtig erkannt
- fast alle von denen, die am weitesten weg waren hat er nicht erkannt
- von den mittelweiten hat er manche nicht erkannt

############################ Mask erstellen ###################################

- Anlehnung an https://github.com/evtimovi/


################################# Meeting 04.11. #############################
die Bilder, die weiter weg sind haben evtl zu viel Rand, deswegen:
=> Bilder zurecht croppen - https://github.com/IDEA-Research/GroundingDINO
- gibt bounding boxes zurück

Problem: wenn das funktioniert kann man vielleicht etwas weiter weggehen, aber nicht weit, da Bilder zu low-res

- hochauflösende Bilder aus dem Internet ziehen

- für Masken erstellen: https://github.com/IDEA-Research/Grounded-SAM-2

###################################################################################################################
########################### Grounding DINO #################################

- verschiedene Prompts ausprobiert. Häufig macht er mehrere bounding boxes. verschiedene Thresholds ausprobiert (35 => 40)
- "traffic sign"
- "Foto"
- "only the Photography"
- "only the Paper with the foto on it" mit Box-Threshold ist perfekt

=> The images with the bounding boxes were cropped and saved as well
=> I classified the cropped images and it was able to classify with 100% accuracy

=> Bilder von ca. 1m Entfernung aufgenommen und das hat auch funktioniert

=> dann habe ich 5 Bilder pro Foto gemacht, wo ich von einer Entfernung von ca. 3 - 0,5m mich auf die Fotos zubewegt habe
=> die Bilder mit magick in PNG umgewandelt (hat ca. 2 min gedauert)
=> die 25 Bilder mit GroundingDINO gecroppt (hat 93s gedauert)
=> dann predicted: auch alles korrekt (100% accuracy)

############################# SAM ##########################################

- Ich habe versucht masken zu generieren, die alles andere außer das Bild maskieren, das hat aber nicht gut funktioniert
- Ich habe versucht immer die Mitte des Bildes zu fokussieren
- hat manchmal funktioniert, oft nicht so

=> ist vermutlich völlig unnötig gewesen, weil ich die digitalen Bilder angreifen muss und die dann ausdrucken.

Mögliches Vorgehen:

1. digital images nehmen
2. mit SAM Masken für diese generieren
3. Robust Adversarial Attacks auf die maskierten Bilder
4. Perturbation-Masken auf Originalgröße hochskalieren und auf die Original-Bilder raufklatschen
5. nochmal ausdrucken und abfotografieren


################################# Ressourcen ################################
adversarial-robustness-toolbox: https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/evasion.html#adversarial-patch-pytorch
1. GRAPHITE (blackbox oder whitebox) https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/examples/get_started_pytorch.py
2. ShapeShifter Attack - Tensorflow
3. Robust DPatch
4. Adversarial Patch

sacct -j 1563435 --format=JobID,State,Elapsed